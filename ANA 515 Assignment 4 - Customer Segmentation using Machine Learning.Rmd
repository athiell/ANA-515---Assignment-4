---
title: "ANA 515 Assignment 4 - Customer Segmentation using Machine Learning"
author: "Amanda Thiell"
date: "5/8/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r install, include=FALSE}
options(repos = list(CRAN="http://cran.rstudio.com/"))
install.packages("ggplot2")
install.packages("plotrix")
install.packages("cluster")
install.packages("gridExtra")
install.packages("grid")
install.packages("factoextra")
install.packages("NbClust")
library(ggplot2)
library(plotrix)
library(cluster)
library(gridExtra)
library(grid)
library(factoextra)
library(NbClust)
```

1. Discuss the business problem/goal

## Customer Segmentation is one the most important applications of unsupervised learning and is an important practise of dividing customers base into individual groups that are similar. It is useful in customized marketing. Using clustering techniques, companies can identify the several segments of customers allowing them to target the potential user base. In this exploration we will use mall customer information to segment the customers based on the age, gender, and interest to improve their individualized marketing and future shopping experiences.

2.	identify where the dataset was retrieved from

## The dataset used for this project was sourced from the website kaggle. Kaggle is a platform utilized by data scientists and machine learning practitioners which allows users to find/publish data sets for public review. The data used for this project can be found at the following link: https://www.kaggle.com/datasets/shwetabh123/mall-customers 

3. Identify the code that imported and saved your dataset in R 

```{r dataset, include=TRUE}
url <- "C:\\Users\\Amanda\\Downloads\\Mall_Customers.csv"
customer_data <- read.csv(url)
```

4. Describe your data set

## Our data contains `r nrow (customer_data)` rows of customer data and including `r ncol (customer_data)` columns of variables. The column names are as follows: `r names(customer_data)`. In addition, below is a breakdown of the percentile breakdown of this dataset.

```{r sd, echo=FALSE}
summary(customer_data)
```

## The standard deviation (SD) of our variables Age, Income, and Spending are as follows:
## SD(age)= `r sd(customer_data$Age)`
## SD(income)= `r sd(customer_data$Annual.Income..k..)`
## SD(spending)= `r  sd(customer_data$Spending.Score..1.100.)`

5.	Discuss any data preparation, missing values and errors

## Prior to actually completing tests on a dataset, it is important to clean the dtata and reorganize as necessary. Due to the level of needed accuracy, it is crucial that any outliers/anomalies, mising values, or inaccurate data are removed/resolved. This ensures there is no skew to the results because of the incorrect data.

## In this specific sample, there are no missing values to be removed. To ensure there are no anomalies, the anomalize function can be used with additional review of the standard deviation of files and their mean value. After searching for incorrect dtaa, we want to see if any reformatting is necessary. One frequently seen example of this is date format - is the associated date in the same format as the others? Ex: MM/DD/YYYY vs MM/YY. Next, data preparation includes combining data. From my prior experience, this is most helpful when one data set does not contain all necessary information for review. Ex - adding a column, creating a subset, combining full datasets. Renaming of the dataset's columns was not necessary in this case since they are pretty self explanatory, but renaming could be completed for ease of reference. For example, the column indicating "gender" is named "genre" ; this could be updated but naming is not an issue as we will correct during the visualization phase. These are all great ways to ensure the data is prepped properly prior to analysis.

6.	Discuss the modeling

## In this machine learning project, we will make use of K-means clustering which is the essential algorithm for clustering unlabeled dataset. K-means clustering is an iterative algorithm that partitions a group of data containing n values into k subgroups. We then find patterns within this data which are present as k-clusters. Each of the n value belongs to the k cluster with the nearest mean. 

7.	produce and discuss the output (see below)
8.	provide explanation with any visuals (see below)

## Data exploration

```{r description, echo=FALSE}
str(customer_data)
names(customer_data)
head(customer_data)
```

## Customer Gender Visualization - bar graph and pie chart

## The following charts show the gender distribution across our customer_data dataset

```{r bar_graph1, echo=FALSE}
a=table(customer_data$Genre)
barplot(a,main="Using BarPlot to display Gender Comparision",
       ylab="Count",
       xlab="Gender",
       col=rainbow(2),
       legend=rownames(a))
```

```{r pie_chart1, echo=FALSE}
pct=round(a/sum(a)*100)
lbs=paste(c("Female","Male")," ",pct,"%",sep=" ")
library(plotrix)
pie3D(a,labels=lbs,
   main="Pie Chart Depicting Ratio of Female and Male")
```

## From the above graph, we conclude that the percentage of females is 56%, whereas the percentage of male in the customer dataset is 44%.

## Visualization of Age Distribution
## Let us plot a histogram to view the distribution to plot the frequency of customer ages. We will first proceed by taking summary of the Age variable.

```{r age_summary, echo=FALSE}
summary(customer_data$Age)
```

```{r age_hist, echo=FALSE}
hist(customer_data$Age,
    col="blue",
    main="Histogram to Show Count of Age Class",
    xlab="Age Class",
    ylab="Frequency",
    labels=TRUE)
```

```{r age_box, echo=FALSE}
boxplot(customer_data$Age,
       col="green",
       main="Boxplot for Descriptive Analysis of Age")
```

## From the above two visualizations, we conclude that the majority of customer ages fall between 30 and 35. The minimum age of customers is 18, whereas, the maximum age is 70. The box plot also shows the inner quartiles range between 30 to 50, with the least amount of customer ages falling in the range of 60 to 65.

## Analysis of annual income of customers
## In this section of the R project, we will create visualizations to analyze the annual income of the customers. We will plot a histogram and then we will proceed to examine this data using a density plot.

```{r income_hist, echo=FALSE}
summary(customer_data$Annual.Income..k..)
hist(customer_data$Annual.Income..k..,
  col="#660033",
  main="Histogram for Annual Income",
  xlab="Annual Income Class",
  ylab="Frequency",
  labels=TRUE)
```

```{r income_density, echo=FALSE}
plot(density(customer_data$Annual.Income..k..),
    col="yellow",
    main="Density Plot for Annual Income",
    xlab="Annual Income Class",
    ylab="Density")
polygon(density(customer_data$Annual.Income..k..),
        col="#ccff66")
```
        
## From the above descriptive analysis, we conclude that the minimum annual income of the customers is 15 (in thousands) and the maximum income is 137 (in thousands). People earning an average income of 70k have the highest frequency count in our histogram distribution. The average salary of all the customers is 60.56k. In the Kernel Density Plot that we displayed above, we observe that the annual income has a normal distribution.
        
## Analyzing Spending Score of the Customers
## In this section of the project, we want to review how the customer spending score is affected by the other variables, and oranize the data accordingly for presentation. To do this, we can use a combination of a box plot and histogram for analysis of distribution and frequency.

```{r sum_score, echo=FALSE}
summary(customer_data$Spending.Score..1.100.)
```

```{r box_score, echo=FALSE}
boxplot(customer_data$Spending.Score..1.100.,
   horizontal=TRUE,
   col="#990000",
   main="BoxPlot for Descriptive Analysis of Spending Score")
```

```{r hist_score, echo=FALSE}
hist(customer_data$Spending.Score..1.100.,
    main="HistoGram for Spending Score",
    xlab="Spending Score Class",
    ylab="Frequency",
    col="#6600cc",
    labels=TRUE)
```

## The minimum spending score is 1, maximum is 99 and the average is 50.20. We can see Descriptive Analysis of Spending Score is that Min is 1, Max is 99 and avg. is 50.20. From the histogram, we conclude that customers between class 40 and 50 have the highest spending score among all the classes.

## K-means Algorithm
## While using the k-means clustering algorithm, the first step is to indicate the number of clusters (k) we are trying to produce. First, k objects from dataset are selected randomly that will serve as the initial centers for our clusters. These are the cluster means (aka centroids). Then, the remaining objects have an assignment of the closest centroid. This centroid is defined by the Euclidean Distance present between the object and the cluster mean (referred to as “cluster assignment”). When the assignment is complete, the algorithm proceeds to calculate new mean value of each cluster present in the data. After the recalculation of the centers, the observations are checked if they are closer to a different cluster. Using the updated cluster mean, then are reassigned as needed. This continues to occur until no updates are necessary through this process.

## To determining the optimal clusters, there are three popular methods – Elbow method, Silhouette method, and Gap statistic

## Elbow method

```{r elbow, echo=FALSE}
library(purrr)
set.seed(123)
# function to calculate total intra-cluster sum of square 
iss <- function(k) {
  kmeans(customer_data[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}

k.values <- 1:10
iss_values <- map_dbl(k.values, iss)
plot(k.values, iss_values,
    type="b", pch = 19, frame = FALSE, 
    xlab="Number of clusters K",
    ylab="Total intra-clusters sum of squares")
```

## Average Silhouette Method

## Using the average silhouette method, we can measure the quality of our clustering operation. We can then determine how well the data object fits within the cluster is the data object. If we obtain a high average silhouette width, it means that we have good clustering. The average silhouette method calculates the mean of silhouette observations for different k values. With the optimal number of k clusters, one can maximize the average silhouette over significant values for k clusters.

```{r silhouette, echo=FALSE}
library(cluster) 
library(gridExtra)
library(grid)
k2<-kmeans(customer_data[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(customer_data[,3:5],"euclidean")))

k3<-kmeans(customer_data[,3:5],3,iter.max=100,nstart=50,algorithm="Lloyd")
s3<-plot(silhouette(k3$cluster,dist(customer_data[,3:5],"euclidean")))

k4<-kmeans(customer_data[,3:5],4,iter.max=100,nstart=50,algorithm="Lloyd")
s4<-plot(silhouette(k4$cluster,dist(customer_data[,3:5],"euclidean")))

k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],"euclidean")))

k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],"euclidean")))

k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))

k8<-kmeans(customer_data[,3:5],8,iter.max=100,nstart=50,algorithm="Lloyd")
s8<-plot(silhouette(k8$cluster,dist(customer_data[,3:5],"euclidean")))

k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],"euclidean")))

k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],"euclidean")))

library(NbClust)
library(factoextra)
fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")
```

## Gap Statistic Method

## Using the gap statistic, one can compare the total intracluster variation for different values of k along with their expected values under the null reference distribution of data. For each variable, we can calculate the range between min(xi) and max (xj) through which we can produce values uniformly from interval lower bound to upper bound.

```{r gap, echo=FALSE}
set.seed(125)
stat_gap <- clusGap(customer_data[,3:5], FUN = kmeans, nstart = 25,
            K.max = 10, B = 50)
fviz_gap_stat(stat_gap)
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
k6
```

## Visualizing the Clustering Results using the First Two Principle Components

```{r pcclust, echo=FALSE}
pcclust=prcomp(customer_data[,3:5],scale=FALSE) #principal component analysis
summary(pcclust)
pcclust$rotation[,1:2]
```

```{r}
set.seed(1)
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```

## From the above visualization, we observe that there is a distribution of 6 clusters as follows –

## Cluster 6 and 4 – These clusters represent the customer_data with the medium income salary as well as the medium annual spend of salary.

## Cluster 1 – This cluster represents the customer_data having a high annual income as well as a high annual spend.

## Cluster 3 – This cluster denotes the customer_data with low annual income as well as low yearly spend of income.

## Cluster 2 – This cluster denotes a high annual income and low yearly spend.

## Cluster 5 – This cluster represents a low annual income but its high yearly expenditure.

```{r kCols, echo=FALSE}
kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}
digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters
plot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab ="K-means",ylab="classes")
legend("bottomleft",unique(dignm),fill=unique(kCols(digCluster)))
```

## Cluster 4 and 1 – These two clusters consist of customers with medium PCA1 and medium PCA2 score.

## Cluster 6 – This cluster represents customers having a high PCA2 and a low PCA1.

## Cluster 5 – In this cluster, there are customers with a medium PCA1 and a low PCA2 score.

## Cluster 3 – This cluster comprises of customers with a high PCA1 income and a high PCA2.

## Cluster 2 – This comprises of customers with a high PCA2 and a medium annual spend of income.

## In conclusion, the colustering techniques reviewed above assist with organizing variables without prior classification. In this case, organizing the characteristics of variables for mall shoppers allows shows to market to these types of customers to improve and expand their sales.